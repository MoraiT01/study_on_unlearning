{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization of Noise Generator\n",
    "\n",
    "We want to find the right Hyperparameters for the Noise Generator.\n",
    "For the Optimization, we will use the `Optuna` Framework\n",
    "\n",
    "The correct Hyperparameters for the Noise will be found for:\n",
    "- `GeFeU`\n",
    "- `GEMU`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Add the parent directory to sys.path\n",
    "sys.path.append(os.path.join('..', 'src'))\n",
    "\n",
    "import gefeu\n",
    "import gemu\n",
    "import mlp_dataclass\n",
    "import metrics\n",
    "from helper import load_models_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## GeFeU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-09 09:34:07,010] Using an existing study with name 'GenOptiGeFeU' instead of creating a new one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using an existing study with name 'GenOptiGeFeU' instead of creating a new one.\n",
      "Using an existing study with name 'GenOptiGeFeU' instead of creating a new one.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import sys\n",
    "import pickle\n",
    "import optuna\n",
    "import os\n",
    "\n",
    "# Add stream handler of stdout to show the messages\n",
    "optuna.logging.get_logger(\"optuna\").addHandler(logging.StreamHandler(sys.stdout))\n",
    "study1_name = \"GenOptiGeFeU\"  # Unique identifier of the study.\n",
    "storage1_name = \"sqlite:///{}.db\".format(study1_name)\n",
    "\n",
    "if os.path.exists(\"sampler_gefeu.pkl\"):\n",
    "    restored1_sampler = pickle.load(open(\"sampler_gefeu.pkl\", \"rb\"))\n",
    "    study_gefeu = optuna.create_study(study_name=study1_name, storage=storage1_name, load_if_exists=True, sampler=restored1_sampler)\n",
    "else:\n",
    "    study_gefeu = optuna.create_study(study_name=study1_name, storage=storage1_name, load_if_exists=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-01-09 09:43:17,396] Trial 15 failed with parameters: {'opt_Epochs': 5, 'opt_Learning_Rate': 0.1057274899858981, 'opt_Batch_Size': 72, 'opt_N2R_Ratio': 19.396307415554414, 'opt_Regularization_term': 0.20168989401505294, 'opt_Noise_Dim': 128, 'opt_Impair_LR': 0.23015044491715436, 'opt_Repair_LR': 0.141910032906416, 'l1': 368, 'l2': 1009, 'l3': 541, 'l4': 903, 'l5': 1007, 'l6': 318, 'l7': 710, 'l8': 990, 'l9': 195, 'n_layers': 2} because of the following error: The value nan is not acceptable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 15 failed with parameters: {'opt_Epochs': 5, 'opt_Learning_Rate': 0.1057274899858981, 'opt_Batch_Size': 72, 'opt_N2R_Ratio': 19.396307415554414, 'opt_Regularization_term': 0.20168989401505294, 'opt_Noise_Dim': 128, 'opt_Impair_LR': 0.23015044491715436, 'opt_Repair_LR': 0.141910032906416, 'l1': 368, 'l2': 1009, 'l3': 541, 'l4': 903, 'l5': 1007, 'l6': 318, 'l7': 710, 'l8': 990, 'l9': 195, 'n_layers': 2} because of the following error: The value nan is not acceptable.\n",
      "Trial 15 failed with parameters: {'opt_Epochs': 5, 'opt_Learning_Rate': 0.1057274899858981, 'opt_Batch_Size': 72, 'opt_N2R_Ratio': 19.396307415554414, 'opt_Regularization_term': 0.20168989401505294, 'opt_Noise_Dim': 128, 'opt_Impair_LR': 0.23015044491715436, 'opt_Repair_LR': 0.141910032906416, 'l1': 368, 'l2': 1009, 'l3': 541, 'l4': 903, 'l5': 1007, 'l6': 318, 'l7': 710, 'l8': 990, 'l9': 195, 'n_layers': 2} because of the following error: The value nan is not acceptable.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-01-09 09:43:17,398] Trial 15 failed with value nan.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 15 failed with value nan.\n",
      "Trial 15 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-09 09:54:37,764] Trial 16 finished with value: 1.2158881768584249 and parameters: {'opt_Epochs': 10, 'opt_Learning_Rate': 0.10894406201518292, 'opt_Batch_Size': 51, 'opt_N2R_Ratio': 18.578874381487278, 'opt_Regularization_term': 0.056790914585226886, 'opt_Noise_Dim': 355, 'opt_Impair_LR': 0.07639295637763363, 'opt_Repair_LR': 0.22081165973501782, 'l1': 424, 'l2': 284, 'l3': 841, 'l4': 832, 'l5': 803, 'l6': 336, 'l7': 366, 'l8': 472, 'l9': 406, 'n_layers': 5}. Best is trial 16 with value: 1.2158881768584249.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 16 finished with value: 1.2158881768584249 and parameters: {'opt_Epochs': 10, 'opt_Learning_Rate': 0.10894406201518292, 'opt_Batch_Size': 51, 'opt_N2R_Ratio': 18.578874381487278, 'opt_Regularization_term': 0.056790914585226886, 'opt_Noise_Dim': 355, 'opt_Impair_LR': 0.07639295637763363, 'opt_Repair_LR': 0.22081165973501782, 'l1': 424, 'l2': 284, 'l3': 841, 'l4': 832, 'l5': 803, 'l6': 336, 'l7': 366, 'l8': 472, 'l9': 406, 'n_layers': 5}. Best is trial 16 with value: 1.2158881768584249.\n",
      "Trial 16 finished with value: 1.2158881768584249 and parameters: {'opt_Epochs': 10, 'opt_Learning_Rate': 0.10894406201518292, 'opt_Batch_Size': 51, 'opt_N2R_Ratio': 18.578874381487278, 'opt_Regularization_term': 0.056790914585226886, 'opt_Noise_Dim': 355, 'opt_Impair_LR': 0.07639295637763363, 'opt_Repair_LR': 0.22081165973501782, 'l1': 424, 'l2': 284, 'l3': 841, 'l4': 832, 'l5': 803, 'l6': 336, 'l7': 366, 'l8': 472, 'l9': 406, 'n_layers': 5}. Best is trial 16 with value: 1.2158881768584249.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-01-09 09:58:17,365] Trial 17 failed with parameters: {'opt_Epochs': 10, 'opt_Learning_Rate': 0.2194577995561068, 'opt_Batch_Size': 162, 'opt_N2R_Ratio': 7.143927641052668, 'opt_Regularization_term': 0.16047532993361482, 'opt_Noise_Dim': 33, 'opt_Impair_LR': 0.025846138686518624, 'opt_Repair_LR': 0.17186523349146535, 'l1': 654, 'l2': 416, 'l3': 271, 'l4': 685, 'l5': 129, 'l6': 69, 'l7': 979, 'l8': 1007, 'l9': 986, 'n_layers': 1} because of the following error: The value nan is not acceptable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 17 failed with parameters: {'opt_Epochs': 10, 'opt_Learning_Rate': 0.2194577995561068, 'opt_Batch_Size': 162, 'opt_N2R_Ratio': 7.143927641052668, 'opt_Regularization_term': 0.16047532993361482, 'opt_Noise_Dim': 33, 'opt_Impair_LR': 0.025846138686518624, 'opt_Repair_LR': 0.17186523349146535, 'l1': 654, 'l2': 416, 'l3': 271, 'l4': 685, 'l5': 129, 'l6': 69, 'l7': 979, 'l8': 1007, 'l9': 986, 'n_layers': 1} because of the following error: The value nan is not acceptable.\n",
      "Trial 17 failed with parameters: {'opt_Epochs': 10, 'opt_Learning_Rate': 0.2194577995561068, 'opt_Batch_Size': 162, 'opt_N2R_Ratio': 7.143927641052668, 'opt_Regularization_term': 0.16047532993361482, 'opt_Noise_Dim': 33, 'opt_Impair_LR': 0.025846138686518624, 'opt_Repair_LR': 0.17186523349146535, 'l1': 654, 'l2': 416, 'l3': 271, 'l4': 685, 'l5': 129, 'l6': 69, 'l7': 979, 'l8': 1007, 'l9': 986, 'n_layers': 1} because of the following error: The value nan is not acceptable.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-01-09 09:58:17,365] Trial 17 failed with value nan.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 17 failed with value nan.\n",
      "Trial 17 failed with value nan.\n"
     ]
    }
   ],
   "source": [
    "def objective(trial):\n",
    "\n",
    "    opt_Epochs = trial.suggest_int('opt_Epochs', 1, 10)\n",
    "    opt_Learning_Rate = trial.suggest_float('opt_Learning_Rate', 0.01, 0.3)\n",
    "    opt_Batch_Size = trial.suggest_int('opt_Batch_Size', 32, 256)\n",
    "    opt_N2R_Ratio = trial.suggest_float('opt_N2R_Ratio', 0.01, 20)\n",
    "    opt_Regularization_term = trial.suggest_float('opt_Regularization_term', 0.01, 0.3)\n",
    "    opt_Noise_Dim = trial.suggest_int('opt_Noise_Dim', 1, 512)\n",
    "    opt_Impair_LR = trial.suggest_float('opt_Impair_LR', 0.01, 0.3)\n",
    "    opt_Repair_LR = trial.suggest_float('opt_Repair_LR', 0.01, 0.3)\n",
    "\n",
    "    l1 = trial.suggest_int('l1', 32, 1024)\n",
    "    l2 = trial.suggest_int('l2', 32, 1024)\n",
    "    l3 = trial.suggest_int('l3', 32, 1024)\n",
    "    l4 = trial.suggest_int('l4', 32, 1024)\n",
    "    l5 = trial.suggest_int('l5', 32, 1024)\n",
    "    l6 = trial.suggest_int('l6', 32, 1024)\n",
    "    l7 = trial.suggest_int('l7', 32, 1024)\n",
    "    l8 = trial.suggest_int('l8', 32, 1024)\n",
    "    l9 = trial.suggest_int('l9', 32, 1024)\n",
    "    n_layers = trial.suggest_int('n_layers', 1, 9)\n",
    "\n",
    "    Layers = [l1, l2, l3, l4, l5, l6, l7, l8, l9]\n",
    "    Layers = Layers[:n_layers]\n",
    "\n",
    "    random_int = random.randint(0, 29)\n",
    "    train = load_models_dict(path=\"../data/models/mnist/all/test_ensemble\")[random_int]\n",
    "    \n",
    "    unlearned = gefeu._main(\n",
    "        model=train,\n",
    "        dataset_name=\"mnist\",\n",
    "        t_Epochs = opt_Epochs,\n",
    "        t_Learning_Rate = opt_Learning_Rate,\n",
    "        t_Batch_Size = opt_Batch_Size,\n",
    "        t_N2R_Ratio= opt_N2R_Ratio,\n",
    "        t_Regularization_term = opt_Regularization_term,\n",
    "        t_Layers = Layers,\n",
    "        t_Noise_Dim = opt_Noise_Dim,\n",
    "        t_Impair_LR=opt_Impair_LR,\n",
    "        t_Repair_LR=opt_Repair_LR,\n",
    "        logs=False,\n",
    "        model_eval_logs=False,\n",
    "    )\n",
    "\n",
    "    valid_ds = mlp_dataclass.MNIST_CostumDataset(\n",
    "        sample_mode=\"all\",\n",
    "        train=False,\n",
    "        test=True,\n",
    "        balanced=False,\n",
    "        dataset_name=\"mnist\",\n",
    "        download=False,\n",
    "    )  \n",
    "    valid_dl = DataLoader(valid_ds, 256, shuffle=False)\n",
    "\n",
    "    random_int = random.randint(0, 29)\n",
    "    exact = load_models_dict(path=\"../data/models/mnist/except_erased/test_ensemble\")[random_int]\n",
    "    \n",
    "    div = metrics.kl_divergence_between_models(\n",
    "        model1 = exact,\n",
    "        model2 = unlearned,\n",
    "        data_loader = valid_dl,\n",
    "    )\n",
    "\n",
    "    return div\n",
    "\n",
    "study_gefeu.optimize(objective, n_trials=3)\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Save the sampler with pickle to be loaded later.\n",
    "with open(\"sampler_gefeu.pkl\", \"wb\") as fout:\n",
    "    pickle.dump(study_gefeu.sampler, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = load_models_dict(path=\"../data/models/mnist/all/test_ensemble\")[1]\n",
    "z = load_models_dict(path=\"../data/models/mnist/except_erased/test_ensemble\")[1]\n",
    "valid_ds = mlp_dataclass.MNIST_CostumDataset(\n",
    "        sample_mode=\"all\",\n",
    "        train=False,\n",
    "        test=True,\n",
    "        balanced=False,\n",
    "        dataset_name=\"mnist\",\n",
    "        download=False,\n",
    "    )  \n",
    "valid_dl = DataLoader(valid_ds, 56, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.04689864512512139"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.kl_divergence_between_models(u, z, valid_dl, device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO be determind\n",
    "\n",
    "# standard_model = src.gefeu._main(\n",
    "#     model=None,\n",
    "#     dataset_name=\"mnist\",\n",
    "#     t_Epochs=,\n",
    "#     t_Batch_Size=,\n",
    "#     t_Learning_Rate=,\n",
    "#     t_N2R_Ratio=,\n",
    "#     t_Regularization_term=,\n",
    "#     t_Layers=,\n",
    "#     t_Noise_Dim=,\n",
    "#     t_Impair_LR=,\n",
    "#     t_Repair_LR=,\n",
    "#     logs=True,\n",
    "#     model_eval_logs=False,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## GEMU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add stream handler of stdout to show the messages\n",
    "optuna.logging.get_logger(\"optuna\").addHandler(logging.StreamHandler(sys.stdout))\n",
    "study2_name = \"GenOptiGEMU\"  # Unique identifier of the study.\n",
    "storage2_name = \"sqlite:///{}.db\".format(study2_name)\n",
    "\n",
    "if os.path.exists(\"sampler_gemu.pkl\"):\n",
    "    restored2_sampler = pickle.load(open(\"sampler_gemu.pkl\", \"rb\"))\n",
    "study_gemu = optuna.create_study(study_name=study2_name, storage=storage2_name, load_if_exists=True, sampler=restored2_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "\n",
    "    opt_Epochs = trial.suggest_int('opt_Epochs', 1, 10)\n",
    "    opt_Learning_Rate = trial.suggest_float('opt_Learning_Rate', 0.01, 0.3)\n",
    "    opt_Batch_Size = trial.suggest_int('opt_Batch_Size', 32, 256)\n",
    "    opt_N2R_Ratio = trial.suggest_float('opt_N2R_Ratio', 0.01, 20)\n",
    "    opt_Regularization_term = trial.suggest_float('opt_Regularization_term', 0.01, 0.3)\n",
    "    opt_Noise_Dim = trial.suggest_int('opt_Noise_Dim', 1, 512)\n",
    "    opt_Impair_LR = trial.suggest_float('opt_Impair_LR', 0.01, 0.3)\n",
    "    opt_Repair_LR = trial.suggest_float('opt_Repair_LR', 0.01, 0.3)\n",
    "\n",
    "    l1 = trial.suggest_int('l1', 32, 1024)\n",
    "    l2 = trial.suggest_int('l2', 32, 1024)\n",
    "    l3 = trial.suggest_int('l3', 32, 1024)\n",
    "    l4 = trial.suggest_int('l4', 32, 1024)\n",
    "    l5 = trial.suggest_int('l5', 32, 1024)\n",
    "    l6 = trial.suggest_int('l6', 32, 1024)\n",
    "    l7 = trial.suggest_int('l7', 32, 1024)\n",
    "    l8 = trial.suggest_int('l8', 32, 1024)\n",
    "    l9 = trial.suggest_int('l9', 32, 1024)\n",
    "    n_layers = trial.suggest_int('n_layers', 1, 9)\n",
    "\n",
    "    Layers = [l1, l2, l3, l4, l5, l6, l7, l8, l9]\n",
    "    Layers = Layers[:n_layers]\n",
    "\n",
    "    random_int = random.randint(0, 29)\n",
    "    train = load_models_dict(path=\"../data/models/mnist/all/test_ensemble\")[random_int]\n",
    "\n",
    "    unlearned = gemu._main(\n",
    "        model=train,\n",
    "        dataset_name=\"mnist\",\n",
    "        t_Epochs = opt_Epochs,\n",
    "        t_Learning_Rate = opt_Learning_Rate,\n",
    "        t_Batch_Size = opt_Batch_Size,\n",
    "        t_N2R_Ratio= opt_N2R_Ratio,\n",
    "        t_Regularization_term = opt_Regularization_term,\n",
    "        t_Layers = Layers,\n",
    "        t_Noise_Dim = opt_Noise_Dim,\n",
    "        t_Impair_LR=opt_Impair_LR,\n",
    "        t_Repair_LR=opt_Repair_LR,\n",
    "        logs=False,\n",
    "        model_eval_logs=False,\n",
    "    )\n",
    "\n",
    "    valid_ds = mlp_dataclass.MNIST_CostumDataset(\n",
    "        sample_mode=\"all\",\n",
    "        train=False,\n",
    "        test=True,\n",
    "        balanced=False,\n",
    "        dataset_name=\"mnist\",\n",
    "        download=False,\n",
    "    )  \n",
    "    valid_dl = DataLoader(valid_ds, 256, shuffle=False)\n",
    "\n",
    "    random_int = random.randint(0, 29)\n",
    "    exact = load_models_dict(path=\"../data/models/mnist/except_erased/test_ensemble\")[random_int]\n",
    "    \n",
    "    div = metrics.kl_divergence_between_models(\n",
    "        model1 = exact,\n",
    "        model2 = unlearned,\n",
    "        data_loader = valid_dl,\n",
    "    )\n",
    "\n",
    "    return div\n",
    "\n",
    "study_gemu.optimize(objective, n_trials=3)\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Save the sampler with pickle to be loaded later.\n",
    "with open(\"sampler_gemu.pkl\", \"wb\") as fout:\n",
    "    pickle.dump(study_gemu.sampler, fout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO be determind\n",
    "\n",
    "# standard_model = src.gefeu._main(\n",
    "#     model=None,\n",
    "#     dataset_name=\"mnist\",\n",
    "#     t_Epochs=,\n",
    "#     t_Batch_Size=,\n",
    "#     t_Learning_Rate=,\n",
    "#     t_N2R_Ratio=,\n",
    "#     t_Regularization_term=,\n",
    "#     t_Layers=,\n",
    "#     t_Noise_Dim=,\n",
    "#     t_Impair_LR=,\n",
    "#     t_Repair_LR=,\n",
    "#     logs=True,\n",
    "#     model_eval_logs=False,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Gradient Ascent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add stream handler of stdout to show the messages\n",
    "optuna.logging.get_logger(\"optuna\").addHandler(logging.StreamHandler(sys.stdout))\n",
    "study3_name = \"OptiGA\"  # Unique identifier of the study.\n",
    "storage3_name = \"sqlite:///{}.db\".format(study2_name)\n",
    "\n",
    "if os.path.exists(\"sampler_ga.pkl\"):\n",
    "    restored3_sampler = pickle.load(open(\"sampler_ga.pkl\", \"rb\"))\n",
    "study_ga = optuna.create_study(study_name=study3_name, storage=storage3_name, load_if_exists=True, sampler=restored3_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unlearning import SimpleGradientAscent\n",
    "\n",
    "def objective(trial):\n",
    "\n",
    "    opt_Epochs = trial.suggest_int('opt_Epochs', 1, 10)\n",
    "    opt_Learning_Rate = trial.suggest_float('opt_Learning_Rate', 0.01, 0.4)\n",
    "    opt_Batch_Size = trial.suggest_int('opt_Batch_Size', 1, 256)\n",
    "\n",
    "    random_int = random.randint(0, 29)\n",
    "    train = load_models_dict(path=\"../data/models/mnist/all/test_ensemble\")[random_int]\n",
    "\n",
    "    forget_ds = mlp_dataclass.MNIST_CostumDataset(\n",
    "        sample_mode=\"only_erased\",\n",
    "        train=True,\n",
    "        test=False,\n",
    "        balanced=False,\n",
    "        dataset_name=\"mnist\",\n",
    "        download=False,\n",
    "    )\n",
    "    forget_dl = valid_dl = DataLoader(forget_ds, opt_Batch_Size, shuffle=False)\n",
    "\n",
    "    unlearned = SimpleGradientAscent(\n",
    "        model=train,\n",
    "        unlearned_data=forget_dl,\n",
    "        dataset_name=\"mnist\",\n",
    "        t_LR = opt_Learning_Rate,\n",
    "        t_Epochs = opt_Epochs,\n",
    "        )\n",
    "\n",
    "    valid_ds = mlp_dataclass.MNIST_CostumDataset(\n",
    "        sample_mode=\"all\",\n",
    "        train=False,\n",
    "        test=True,\n",
    "        balanced=False,\n",
    "        dataset_name=\"mnist\",\n",
    "        download=False,\n",
    "    )  \n",
    "    valid_dl = DataLoader(valid_ds, 256, shuffle=False)\n",
    "\n",
    "    random_int = random.randint(0, 29)\n",
    "    exact = load_models_dict(path=\"../data/models/mnist/except_erased/test_ensemble\")[random_int]\n",
    "    \n",
    "    div = metrics.kl_divergence_between_models(\n",
    "        model1 = exact,\n",
    "        model2 = unlearned,\n",
    "        data_loader = valid_dl,\n",
    "    )\n",
    "\n",
    "    return div\n",
    "\n",
    "study_gemu.optimize(objective, n_trials=3)\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Save the sampler with pickle to be loaded later.\n",
    "with open(\"sampler_gemu.pkl\", \"wb\") as fout:\n",
    "    pickle.dump(study_gemu.sampler, fout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO be determind\n",
    "\n",
    "# forget_ds = mlp_dataclass.MNIST_CostumDataset(\n",
    "#     sample_mode=\"only_erased\",\n",
    "#     train=True,\n",
    "#     test=False,\n",
    "#     balanced=False,\n",
    "#     dataset_name=\"mnist\",\n",
    "#     download=False,\n",
    "# )\n",
    "# forget_dl = valid_dl = DataLoader(forget_ds, , shuffle=False)\n",
    "\n",
    "# unlearned = SimpleGradientAscent(\n",
    "#     model=train,\n",
    "#     unlearned_data=forget_dl,\n",
    "#     dataset_name=\"mnist\",\n",
    "#     t_LR =,\n",
    "#     t_Epochs =,\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bach.conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
