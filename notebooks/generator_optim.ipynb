{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization of Noise Generator\n",
    "\n",
    "We want to find the right Hyperparameters for the Noise Generator.\n",
    "For the Optimization, we will use the `Optuna` Framework\n",
    "\n",
    "The correct Hyperparameters for the Noise will be found for:\n",
    "- `GeFeU`\n",
    "- `GEMU`\n",
    "- `Gradient Ascent`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Add the parent directory to sys.path\n",
    "sys.path.append(os.path.join('..', 'src'))\n",
    "\n",
    "import gefeu\n",
    "import gemu\n",
    "import mlp_dataclass\n",
    "import metrics\n",
    "from helper import load_models_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## GeFeU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "import pickle\n",
    "import optuna\n",
    "import os\n",
    "\n",
    "# Add stream handler of stdout to show the messages\n",
    "optuna.logging.get_logger(\"optuna\").addHandler(logging.StreamHandler(sys.stdout))\n",
    "study1_name = \"GenOptiGeFeU\"  # Unique identifier of the study.\n",
    "storage1_name = \"sqlite:///{}.db\".format(\"HP_Opti\")\n",
    "\n",
    "if os.path.exists(\"sampler_gefeu.pkl\"):\n",
    "    restored1_sampler = pickle.load(open(\"sampler_gefeu.pkl\", \"rb\"))\n",
    "    study_gefeu = optuna.create_study(study_name=study1_name, storage=storage1_name, load_if_exists=True, sampler=restored1_sampler)\n",
    "else:\n",
    "    study_gefeu = optuna.create_study(study_name=study1_name, storage=storage1_name, load_if_exists=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "\n",
    "    opt_Epochs = trial.suggest_int('opt_Epochs', 1, 10)\n",
    "    opt_Learning_Rate = trial.suggest_float('opt_Learning_Rate', 0.01, 0.3)\n",
    "    opt_Batch_Size = trial.suggest_int('opt_Batch_Size', 32, 256)\n",
    "    opt_N2R_Ratio = trial.suggest_float('opt_N2R_Ratio', 0.01, 20)\n",
    "    opt_Regularization_term = trial.suggest_float('opt_Regularization_term', 0.01, 0.3)\n",
    "    opt_Noise_Dim = trial.suggest_int('opt_Noise_Dim', 1, 512)\n",
    "    opt_Impair_LR = trial.suggest_float('opt_Impair_LR', 0.01, 0.3)\n",
    "    opt_Repair_LR = trial.suggest_float('opt_Repair_LR', 0.01, 0.3)\n",
    "\n",
    "    n_layers = trial.suggest_int('n_layers', 1, 10)\n",
    "\n",
    "    Layers = [1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024]\n",
    "    Layers = Layers[:n_layers]\n",
    "\n",
    "    random_int = random.randint(0, 29)\n",
    "    train = load_models_dict(path=\"../data/models/mnist/all/test_ensemble\")[random_int]\n",
    "    \n",
    "    unlearned = gefeu._main(\n",
    "        model=train,\n",
    "        dataset_name=\"mnist\",\n",
    "        t_Epochs = opt_Epochs,\n",
    "        t_Learning_Rate = opt_Learning_Rate,\n",
    "        t_Batch_Size = opt_Batch_Size,\n",
    "        t_N2R_Ratio= opt_N2R_Ratio,\n",
    "        t_Regularization_term = opt_Regularization_term,\n",
    "        t_Layers = Layers,\n",
    "        t_Noise_Dim = opt_Noise_Dim,\n",
    "        t_Impair_LR=opt_Impair_LR,\n",
    "        t_Repair_LR=opt_Repair_LR,\n",
    "        logs=False,\n",
    "        model_eval_logs=False,\n",
    "    )\n",
    "\n",
    "    valid_ds = mlp_dataclass.MNIST_CostumDataset(\n",
    "        sample_mode=\"all\",\n",
    "        train=True,\n",
    "        test=False,\n",
    "        balanced=False,\n",
    "        dataset_name=\"mnist\",\n",
    "        download=False,\n",
    "    )  \n",
    "    valid_dl = DataLoader(valid_ds, 256, shuffle=False)\n",
    "\n",
    "    random_int = random.randint(0, 29)\n",
    "    exact = load_models_dict(path=\"../data/models/mnist/except_erased/test_ensemble\")[random_int]\n",
    "    \n",
    "    div = metrics.kl_divergence_between_models(\n",
    "        model1 = exact,\n",
    "        model2 = unlearned,\n",
    "        data_loader = valid_dl,\n",
    "    )\n",
    "\n",
    "    return div\n",
    "\n",
    "study_gefeu.optimize(objective, n_trials=200)\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Save the sampler with pickle to be loaded later.\n",
    "with open(\"sampler_gefeu.pkl\", \"wb\") as fout:\n",
    "    pickle.dump(study_gefeu.sampler, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = load_models_dict(path=\"../data/models/mnist/all/test_ensemble\")[1]\n",
    "z = load_models_dict(path=\"../data/models/mnist/except_erased/test_ensemble\")[1]\n",
    "valid_ds = mlp_dataclass.MNIST_CostumDataset(\n",
    "        sample_mode=\"all\",\n",
    "        train=False,\n",
    "        test=True,\n",
    "        balanced=False,\n",
    "        dataset_name=\"mnist\",\n",
    "        download=False,\n",
    "    )  \n",
    "valid_dl = DataLoader(valid_ds, 56, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.kl_divergence_between_models(z, u, valid_dl, device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import gefeu\n",
    "\n",
    "standard_model_gefeu = gefeu._main(\n",
    "    model=u[0],\n",
    "    dataset_name=\"mnist\",\n",
    "    t_Epochs=study_gefeu.best_params[\"opt_Epochs\"],\n",
    "    t_Batch_Size=study_gefeu.best_params[\"opt_Batch_Size\"],\n",
    "    t_Learning_Rate=study_gefeu.best_params[\"opt_Learning_Rate\"],\n",
    "    t_N2R_Ratio=study_gefeu.best_params[\"opt_N2R_Ratio\"],\n",
    "    t_Regularization_term=study_gefeu.best_params[\"opt_Regularization_term\"],\n",
    "    t_Layers=[1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024][:study_gefeu.best_params[\"n_layers\"]],\n",
    "    t_Noise_Dim=study_gefeu.best_params[\"opt_Noise_Dim\"],\n",
    "    t_Impair_LR=study_gefeu.best_params[\"opt_Impair_LR\"],\n",
    "    t_Repair_LR=study_gefeu.best_params[\"opt_Repair_LR\"],\n",
    "    logs=True,\n",
    "    model_eval_logs=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## GEMU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "import pickle\n",
    "import optuna\n",
    "import os\n",
    "\n",
    "\n",
    "# Add stream handler of stdout to show the messages\n",
    "optuna.logging.get_logger(\"optuna\").addHandler(logging.StreamHandler(sys.stdout))\n",
    "study2_name = \"GenOptiGEMU\"  # Unique identifier of the study.\n",
    "storage2_name = \"sqlite:///{}.db\".format(\"HP_Opti\")\n",
    "\n",
    "if os.path.exists(\"sampler_gemu.pkl\"):\n",
    "    restored2_sampler = pickle.load(open(\"sampler_gemu.pkl\", \"rb\"))\n",
    "    study_gemu = optuna.create_study(study_name=study2_name, storage=storage2_name, load_if_exists=True, sampler=restored2_sampler)\n",
    "else:\n",
    "    study_gemu = optuna.create_study(study_name=study2_name, storage=storage2_name, load_if_exists=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "\n",
    "    opt_Epochs = trial.suggest_int('opt_Epochs', 1, 10)\n",
    "    opt_Learning_Rate = trial.suggest_float('opt_Learning_Rate', 0.01, 0.3)\n",
    "    opt_Batch_Size = trial.suggest_int('opt_Batch_Size', 32, 256)\n",
    "    opt_N2R_Ratio = trial.suggest_float('opt_N2R_Ratio', 0.01, 20)\n",
    "    opt_Regularization_term = trial.suggest_float('opt_Regularization_term', 0.01, 0.3)\n",
    "    opt_Noise_Dim = trial.suggest_int('opt_Noise_Dim', 1, 512)\n",
    "    opt_Impair_LR = trial.suggest_float('opt_Impair_LR', 0.01, 0.3)\n",
    "    opt_Repair_LR = trial.suggest_float('opt_Repair_LR', 0.01, 0.3)\n",
    "\n",
    "    n_layers = trial.suggest_int('n_layers', 1, 10)\n",
    "\n",
    "    Layers = [1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024]\n",
    "    Layers = Layers[:n_layers]\n",
    "\n",
    "    random_int = random.randint(0, 29)\n",
    "    train = load_models_dict(path=\"../data/models/mnist/all/test_ensemble\")[random_int]\n",
    "\n",
    "    unlearned = gemu._main(\n",
    "        model=train,\n",
    "        dataset_name=\"mnist\",\n",
    "        t_Epochs = opt_Epochs,\n",
    "        t_Learning_Rate = opt_Learning_Rate,\n",
    "        t_Batch_Size = opt_Batch_Size,\n",
    "        t_N2R_Ratio= opt_N2R_Ratio,\n",
    "        t_Regularization_term = opt_Regularization_term,\n",
    "        t_Layers = Layers,\n",
    "        t_Noise_Dim = opt_Noise_Dim,\n",
    "        t_Impair_LR=opt_Impair_LR,\n",
    "        t_Repair_LR=opt_Repair_LR,\n",
    "        logs=False,\n",
    "        model_eval_logs=False,\n",
    "    )\n",
    "\n",
    "    valid_ds = mlp_dataclass.MNIST_CostumDataset(\n",
    "        sample_mode=\"all\",\n",
    "        train=True,\n",
    "        test=False,\n",
    "        balanced=False,\n",
    "        dataset_name=\"mnist\",\n",
    "        download=False,\n",
    "    )  \n",
    "    valid_dl = DataLoader(valid_ds, 256, shuffle=False)\n",
    "\n",
    "    random_int = random.randint(0, 29)\n",
    "    exact = load_models_dict(path=\"../data/models/mnist/except_erased/test_ensemble\")[random_int]\n",
    "    \n",
    "    div = metrics.kl_divergence_between_models(\n",
    "        model1 = exact,\n",
    "        model2 = unlearned,\n",
    "        data_loader = valid_dl,\n",
    "    )\n",
    "\n",
    "    return div\n",
    "\n",
    "study_gemu.optimize(objective, n_trials=200)\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Save the sampler with pickle to be loaded later.\n",
    "with open(\"sampler_gemu.pkl\", \"wb\") as fout:\n",
    "    pickle.dump(study_gemu.sampler, fout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import gemu\n",
    "\n",
    "standard_model_gemu = gemu._main(\n",
    "    model=u[0],\n",
    "    dataset_name=\"mnist\",\n",
    "    t_Epochs=study_gemu.best_params[\"opt_Epochs\"],\n",
    "    t_Batch_Size=study_gemu.best_params[\"opt_Batch_Size\"],\n",
    "    t_Learning_Rate=study_gemu.best_params[\"opt_Learning_Rate\"],\n",
    "    t_N2R_Ratio=study_gemu.best_params[\"opt_N2R_Ratio\"],\n",
    "    t_Regularization_term=study_gemu.best_params[\"opt_Regularization_term\"],\n",
    "    t_Layers=[1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024][:study_gemu.best_params[\"n_layers\"]],\n",
    "    t_Noise_Dim=study_gemu.best_params[\"opt_Noise_Dim\"],\n",
    "    t_Impair_LR=study_gemu.best_params[\"opt_Impair_LR\"],\n",
    "    t_Repair_LR=study_gemu.best_params[\"opt_Repair_LR\"],\n",
    "    logs=True,\n",
    "    model_eval_logs=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Gradient Ascent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "import pickle\n",
    "import optuna\n",
    "import os\n",
    "\n",
    "\n",
    "# Add stream handler of stdout to show the messages\n",
    "optuna.logging.get_logger(\"optuna\").addHandler(logging.StreamHandler(sys.stdout))\n",
    "study3_name = \"OptiGA\"  # Unique identifier of the study.\n",
    "storage3_name = \"sqlite:///{}.db\".format(\"HP_Opti\")\n",
    "\n",
    "if os.path.exists(\"sampler_ga.pkl\"):\n",
    "    restored3_sampler = pickle.load(open(\"sampler_ga.pkl\", \"rb\"))\n",
    "    study_ga = optuna.create_study(study_name=study3_name, storage=storage3_name, load_if_exists=True, sampler=restored3_sampler)\n",
    "else:\n",
    "    study_ga = optuna.create_study(study_name=study3_name, storage=storage3_name, load_if_exists=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unlearning import SimpleGradientAscent\n",
    "\n",
    "def objective(trial):\n",
    "\n",
    "    opt_Epochs = trial.suggest_int('opt_Epochs', 1, 10)\n",
    "    opt_Learning_Rate = trial.suggest_float('opt_Learning_Rate', 0, 1)\n",
    "    opt_Batch_Size = trial.suggest_int('opt_Batch_Size', 1, 256)\n",
    "\n",
    "    random_int = random.randint(0, 29)\n",
    "    train = load_models_dict(path=\"../data/models/mnist/all/test_ensemble\")[random_int]\n",
    "\n",
    "    forget_ds = mlp_dataclass.MNIST_CostumDataset(\n",
    "        sample_mode=\"only_erased\",\n",
    "        train=True,\n",
    "        test=False,\n",
    "        balanced=False,\n",
    "        dataset_name=\"mnist\",\n",
    "        download=False,\n",
    "    )\n",
    "    forget_dl = valid_dl = DataLoader(forget_ds, opt_Batch_Size, shuffle=False)\n",
    "\n",
    "    unlearned = SimpleGradientAscent(\n",
    "        model=train,\n",
    "        unlearned_data=forget_dl,\n",
    "        dataset_name=\"mnist\",\n",
    "        t_LR = opt_Learning_Rate,\n",
    "        t_Epochs = opt_Epochs,\n",
    "        ).unlearn()\n",
    "\n",
    "    valid_ds = mlp_dataclass.MNIST_CostumDataset(\n",
    "        sample_mode=\"all\",\n",
    "        train=True,\n",
    "        test=False,\n",
    "        balanced=False,\n",
    "        dataset_name=\"mnist\",\n",
    "        download=False,\n",
    "    )  \n",
    "    valid_dl = DataLoader(valid_ds, 256, shuffle=False)\n",
    "\n",
    "    random_int = random.randint(0, 29)\n",
    "    exact = load_models_dict(path=\"../data/models/mnist/except_erased/test_ensemble\")[random_int]\n",
    "    \n",
    "    div = metrics.kl_divergence_between_models(\n",
    "        model1 = exact,\n",
    "        model2 = unlearned,\n",
    "        data_loader = valid_dl,\n",
    "    )\n",
    "\n",
    "    return div\n",
    "\n",
    "study_ga.optimize(objective, n_trials=200)\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Save the sampler with pickle to be loaded later.\n",
    "with open(\"sampler_ga.pkl\", \"wb\") as fout:\n",
    "    pickle.dump(study_ga.sampler, fout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forget_ds = mlp_dataclass.MNIST_CostumDataset(\n",
    "    sample_mode=\"only_erased\",\n",
    "    train=True,\n",
    "    test=False,\n",
    "    balanced=False,\n",
    "    dataset_name=\"mnist\",\n",
    "    download=False,\n",
    ")\n",
    "forget_dl = DataLoader(forget_ds, batch_size=study_ga.best_params[\"opt_Batch_Size\"], shuffle=False)\n",
    "\n",
    "unlearned_model_ga = SimpleGradientAscent(\n",
    "    model=u[0],\n",
    "    unlearned_data=forget_dl,\n",
    "    dataset_name=\"mnist\",\n",
    "    t_LR = study_ga.best_params[\"opt_Learning_Rate\"],\n",
    "    t_Epochs = study_ga.best_params[\"opt_Epochs\"],\n",
    "    ).unlearn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bach.conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
