{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization of Noise Generator\n",
    "\n",
    "We want to find the right Hyperparameters for the Noise Generator.\n",
    "For the Optimization, we will use the `Optuna` Framework\n",
    "\n",
    "The correct Hyperparameters for the Noise will be found for:\n",
    "- `GeFeU`\n",
    "- `GEMU`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Add the parent directory to sys.path\n",
    "sys.path.append(os.path.join('..', 'src'))\n",
    "\n",
    "import gefeu\n",
    "import gemu\n",
    "import mlp_dataclass\n",
    "import metrics\n",
    "from helper import load_models_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## GeFeU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-08 17:26:36,769] Using an existing study with name 'GenOptiGeFeU' instead of creating a new one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using an existing study with name 'GenOptiGeFeU' instead of creating a new one.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import sys\n",
    "import pickle\n",
    "import optuna\n",
    "import os\n",
    "\n",
    "# Add stream handler of stdout to show the messages\n",
    "optuna.logging.get_logger(\"optuna\").addHandler(logging.StreamHandler(sys.stdout))\n",
    "study1_name = \"GenOptiGeFeU\"  # Unique identifier of the study.\n",
    "storage1_name = \"sqlite:///{}.db\".format(study1_name)\n",
    "\n",
    "if os.path.exists(\"sampler_gefeu.pkl\"):\n",
    "    restored1_sampler = pickle.load(open(\"sampler_gefeu.pkl\", \"rb\"))\n",
    "    study_gefeu = optuna.create_study(study_name=study1_name, storage=storage1_name, load_if_exists=True, sampler=restored1_sampler)\n",
    "else:\n",
    "    study_gefeu = optuna.create_study(study_name=study1_name, storage=storage1_name, load_if_exists=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-01-08 17:29:13,447] Trial 9 failed with parameters: {'opt_Epochs': 10, 'opt_Learning_Rate': 0.23748483203027362, 'opt_Batch_Size': 65, 'opt_N2R_Ratio': 19.38557923428422, 'opt_Regularization_term': 0.24958432292577604, 'opt_Noise_Dim': 160, 'opt_Impair_LR': 0.25698894507017267, 'opt_Repair_LR': 0.146004795612318, 'l1': 157, 'l2': 408, 'l3': 801, 'l4': 439, 'l5': 999, 'l6': 982, 'l7': 358, 'l8': 938, 'l9': 259, 'n_layers': 9} because of the following error: The value nan is not acceptable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 9 failed with parameters: {'opt_Epochs': 10, 'opt_Learning_Rate': 0.23748483203027362, 'opt_Batch_Size': 65, 'opt_N2R_Ratio': 19.38557923428422, 'opt_Regularization_term': 0.24958432292577604, 'opt_Noise_Dim': 160, 'opt_Impair_LR': 0.25698894507017267, 'opt_Repair_LR': 0.146004795612318, 'l1': 157, 'l2': 408, 'l3': 801, 'l4': 439, 'l5': 999, 'l6': 982, 'l7': 358, 'l8': 938, 'l9': 259, 'n_layers': 9} because of the following error: The value nan is not acceptable.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-01-08 17:29:13,447] Trial 9 failed with value nan.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 9 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-01-08 17:30:39,494] Trial 10 failed with parameters: {'opt_Epochs': 2, 'opt_Learning_Rate': 0.21294084437549102, 'opt_Batch_Size': 151, 'opt_N2R_Ratio': 16.620943314445984, 'opt_Regularization_term': 0.07861805166230335, 'opt_Noise_Dim': 233, 'opt_Impair_LR': 0.18494713160029108, 'opt_Repair_LR': 0.20806078762507987, 'l1': 700, 'l2': 47, 'l3': 34, 'l4': 103, 'l5': 782, 'l6': 856, 'l7': 722, 'l8': 273, 'l9': 938, 'n_layers': 9} because of the following error: The value nan is not acceptable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 failed with parameters: {'opt_Epochs': 2, 'opt_Learning_Rate': 0.21294084437549102, 'opt_Batch_Size': 151, 'opt_N2R_Ratio': 16.620943314445984, 'opt_Regularization_term': 0.07861805166230335, 'opt_Noise_Dim': 233, 'opt_Impair_LR': 0.18494713160029108, 'opt_Repair_LR': 0.20806078762507987, 'l1': 700, 'l2': 47, 'l3': 34, 'l4': 103, 'l5': 782, 'l6': 856, 'l7': 722, 'l8': 273, 'l9': 938, 'n_layers': 9} because of the following error: The value nan is not acceptable.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-01-08 17:30:39,495] Trial 10 failed with value nan.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 failed with value nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-01-08 17:31:09,363] Trial 11 failed with parameters: {'opt_Epochs': 1, 'opt_Learning_Rate': 0.031038515637847423, 'opt_Batch_Size': 115, 'opt_N2R_Ratio': 4.176981079294669, 'opt_Regularization_term': 0.16913230505811241, 'opt_Noise_Dim': 404, 'opt_Impair_LR': 0.15078688621431519, 'opt_Repair_LR': 0.022262680830003136, 'l1': 382, 'l2': 757, 'l3': 401, 'l4': 169, 'l5': 914, 'l6': 506, 'l7': 839, 'l8': 92, 'l9': 746, 'n_layers': 6} because of the following error: The value nan is not acceptable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 11 failed with parameters: {'opt_Epochs': 1, 'opt_Learning_Rate': 0.031038515637847423, 'opt_Batch_Size': 115, 'opt_N2R_Ratio': 4.176981079294669, 'opt_Regularization_term': 0.16913230505811241, 'opt_Noise_Dim': 404, 'opt_Impair_LR': 0.15078688621431519, 'opt_Repair_LR': 0.022262680830003136, 'l1': 382, 'l2': 757, 'l3': 401, 'l4': 169, 'l5': 914, 'l6': 506, 'l7': 839, 'l8': 92, 'l9': 746, 'n_layers': 6} because of the following error: The value nan is not acceptable.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-01-08 17:31:09,364] Trial 11 failed with value nan.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 11 failed with value nan.\n"
     ]
    }
   ],
   "source": [
    "def objective(trial):\n",
    "\n",
    "    opt_Epochs = trial.suggest_int('opt_Epochs', 1, 10)\n",
    "    opt_Learning_Rate = trial.suggest_float('opt_Learning_Rate', 0.01, 0.3)\n",
    "    opt_Batch_Size = trial.suggest_int('opt_Batch_Size', 32, 256)\n",
    "    opt_N2R_Ratio = trial.suggest_float('opt_N2R_Ratio', 0.01, 20)\n",
    "    opt_Regularization_term = trial.suggest_float('opt_Regularization_term', 0.01, 0.3)\n",
    "    opt_Noise_Dim = trial.suggest_int('opt_Noise_Dim', 1, 512)\n",
    "    opt_Impair_LR = trial.suggest_float('opt_Impair_LR', 0.01, 0.3)\n",
    "    opt_Repair_LR = trial.suggest_float('opt_Repair_LR', 0.01, 0.3)\n",
    "\n",
    "    # print(f\"Epochs: {opt_Epochs} \n",
    "    #       |\\nLearning Rate: {opt_Learning_Rate} \n",
    "    #       |\\nBatch Size: {opt_Batch_Size} \n",
    "    #       |\\nN2R Ratio: {opt_N2R_Ratio} \n",
    "    #       |\\nRegularization Term: {opt_Regularization_term} \n",
    "    #       |\\nNoise Dim: {opt_Noise_Dim}\n",
    "    #       |\\nImpair LR: {opt_Impair_LR}\n",
    "    #       |\\nRepair LR: {opt_Repair_LR}\")\n",
    "\n",
    "    l1 = trial.suggest_int('l1', 32, 1024)\n",
    "    l2 = trial.suggest_int('l2', 32, 1024)\n",
    "    l3 = trial.suggest_int('l3', 32, 1024)\n",
    "    l4 = trial.suggest_int('l4', 32, 1024)\n",
    "    l5 = trial.suggest_int('l5', 32, 1024)\n",
    "    l6 = trial.suggest_int('l6', 32, 1024)\n",
    "    l7 = trial.suggest_int('l7', 32, 1024)\n",
    "    l8 = trial.suggest_int('l8', 32, 1024)\n",
    "    l9 = trial.suggest_int('l9', 32, 1024)\n",
    "    n_layers = trial.suggest_int('n_layers', 1, 9)\n",
    "\n",
    "    Layers = [l1, l2, l3, l4, l5, l6, l7, l8, l9]\n",
    "    Layers = Layers[:n_layers]\n",
    "    # print(\"Layers: \", Layers)\n",
    "\n",
    "    random_int = random.randint(0, 29)\n",
    "    train = load_models_dict(path=\"../data/models/mnist/all/test_ensemble\")[random_int]\n",
    "    \n",
    "    unlearned = gefeu._main(\n",
    "        model=train,\n",
    "        dataset_name=\"mnist\",\n",
    "        t_Epochs = opt_Epochs,\n",
    "        t_Learning_Rate = opt_Learning_Rate,\n",
    "        t_Batch_Size = opt_Batch_Size,\n",
    "        t_N2R_Ratio= opt_N2R_Ratio,\n",
    "        t_Regularization_term = opt_Regularization_term,\n",
    "        t_Layers = Layers,\n",
    "        t_Noise_Dim = opt_Noise_Dim,\n",
    "        t_Impair_LR=opt_Impair_LR,\n",
    "        t_Repair_LR=opt_Repair_LR,\n",
    "        logs=False,\n",
    "        model_eval_logs=False,\n",
    "    )\n",
    "\n",
    "    valid_ds = mlp_dataclass.MNIST_CostumDataset(\n",
    "        sample_mode=\"all\",\n",
    "        train=False,\n",
    "        test=True,\n",
    "        balanced=False,\n",
    "        dataset_name=\"mnist\",\n",
    "        download=False,\n",
    "    )  \n",
    "    valid_dl = DataLoader(valid_ds, 256, shuffle=False)\n",
    "\n",
    "    random_int = random.randint(0, 29)\n",
    "    exact = load_models_dict(path=\"../data/models/mnist/except_erased/test_ensemble\")[random_int]\n",
    "    \n",
    "    div = metrics.kl_divergence_between_models(\n",
    "        model1 = unlearned,\n",
    "        model2 = exact,\n",
    "        data_loader = valid_dl,\n",
    "    )\n",
    "\n",
    "    return div\n",
    "\n",
    "study_gefeu.optimize(objective, n_trials=3)\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Save the sampler with pickle to be loaded later.\n",
    "with open(\"sampler_gefeu.pkl\", \"wb\") as fout:\n",
    "    pickle.dump(study_gefeu.sampler, fout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard_model = src.gefeu._main(\n",
    "#     model=None,\n",
    "#     dataset_name=\"mnist\",\n",
    "#     t_Epochs=,\n",
    "#     t_Batch_Size=,\n",
    "#     t_Learning_Rate=,\n",
    "#     t_N2R_Ratio=,\n",
    "#     t_Regularization_term=,\n",
    "#     t_Layers=,\n",
    "#     t_Noise_Dim=,\n",
    "#     t_Impair_LR=,\n",
    "#     t_Repair_LR=,\n",
    "#     logs=True,\n",
    "#     model_eval_logs=False,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## GEMU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add stream handler of stdout to show the messages\n",
    "optuna.logging.get_logger(\"optuna\").addHandler(logging.StreamHandler(sys.stdout))\n",
    "study2_name = \"GenOptiGEMU\"  # Unique identifier of the study.\n",
    "storage2_name = \"sqlite:///{}.db\".format(study2_name)\n",
    "\n",
    "if os.path.exists(\"sampler_gemu.pkl\"):\n",
    "    restored2_sampler = pickle.load(open(\"sampler_gemu.pkl\", \"rb\"))\n",
    "study_gemu = optuna.create_study(study_name=study2_name, storage=storage2_name, load_if_exists=True, sampler=restored2_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "\n",
    "    opt_Epochs = trial.suggest_int('opt_Epochs', 1, 10)\n",
    "    opt_Learning_Rate = trial.suggest_float('opt_Learning_Rate', 0.01, 0.3)\n",
    "    opt_Batch_Size = trial.suggest_int('opt_Batch_Size', 32, 256)\n",
    "    opt_N2R_Ratio = trial.suggest_float('opt_N2R_Ratio', 0.01, 20)\n",
    "    opt_Regularization_term = trial.suggest_float('opt_Regularization_term', 0.01, 0.3)\n",
    "    opt_Noise_Dim = trial.suggest_int('opt_Noise_Dim', 1, 512)\n",
    "    opt_Impair_LR = trial.suggest_float('opt_Impair_LR', 0.01, 0.3)\n",
    "    opt_Repair_LR = trial.suggest_float('opt_Repair_LR', 0.01, 0.3)\n",
    "\n",
    "    # print(f\"Epochs: {opt_Epochs} \n",
    "    #       |\\nLearning Rate: {opt_Learning_Rate} \n",
    "    #       |\\nBatch Size: {opt_Batch_Size} \n",
    "    #       |\\nN2R Ratio: {opt_N2R_Ratio} \n",
    "    #       |\\nRegularization Term: {opt_Regularization_term} \n",
    "    #       |\\nNoise Dim: {opt_Noise_Dim}\n",
    "    #       |\\nImpair LR: {opt_Impair_LR}\n",
    "    #       |\\nRepair LR: {opt_Repair_LR}\")\n",
    "\n",
    "    l1 = trial.suggest_int('l1', 32, 1024)\n",
    "    l2 = trial.suggest_int('l2', 32, 1024)\n",
    "    l3 = trial.suggest_int('l3', 32, 1024)\n",
    "    l4 = trial.suggest_int('l4', 32, 1024)\n",
    "    l5 = trial.suggest_int('l5', 32, 1024)\n",
    "    l6 = trial.suggest_int('l6', 32, 1024)\n",
    "    l7 = trial.suggest_int('l7', 32, 1024)\n",
    "    l8 = trial.suggest_int('l8', 32, 1024)\n",
    "    l9 = trial.suggest_int('l9', 32, 1024)\n",
    "    n_layers = trial.suggest_int('n_layers', 1, 9)\n",
    "\n",
    "    Layers = [l1, l2, l3, l4, l5, l6, l7, l8, l9]\n",
    "    Layers = Layers[:n_layers]\n",
    "    # print(\"Layers: \", Layers)\n",
    "\n",
    "    random_int = random.randint(0, 29)\n",
    "    train = load_models_dict(path=\"../data/models/mnist/all/test_ensemble\")[random_int]\n",
    "\n",
    "    unlearned = gemu._main(\n",
    "        model=train,\n",
    "        dataset_name=\"mnist\",\n",
    "        t_Epochs = opt_Epochs,\n",
    "        t_Learning_Rate = opt_Learning_Rate,\n",
    "        t_Batch_Size = opt_Batch_Size,\n",
    "        t_N2R_Ratio= opt_N2R_Ratio,\n",
    "        t_Regularization_term = opt_Regularization_term,\n",
    "        t_Layers = Layers,\n",
    "        t_Noise_Dim = opt_Noise_Dim,\n",
    "        t_Impair_LR=opt_Impair_LR,\n",
    "        t_Repair_LR=opt_Repair_LR,\n",
    "        logs=False,\n",
    "        model_eval_logs=False,\n",
    "    )\n",
    "\n",
    "    valid_ds = mlp_dataclass.MNIST_CostumDataset(\n",
    "        sample_mode=\"all\",\n",
    "        train=False,\n",
    "        test=True,\n",
    "        balanced=False,\n",
    "        dataset_name=\"mnist\",\n",
    "        download=False,\n",
    "    )  \n",
    "    valid_dl = DataLoader(valid_ds, 256, shuffle=False)\n",
    "\n",
    "    random_int = random.randint(0, 29)\n",
    "    exact = load_models_dict(path=\"../data/models/mnist/except_erased/test_ensemble\")[random_int]\n",
    "    \n",
    "    div = metrics.kl_divergence_between_models(\n",
    "        model1 = unlearned,\n",
    "        model2 = exact,\n",
    "        data_loader = valid_dl,\n",
    "    )\n",
    "\n",
    "    return div\n",
    "\n",
    "study_gemu.optimize(objective, n_trials=3)\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Save the sampler with pickle to be loaded later.\n",
    "with open(\"sampler_gemu.pkl\", \"wb\") as fout:\n",
    "    pickle.dump(study_gemu.sampler, fout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard_model = src.gefeu._main(\n",
    "#     model=None,\n",
    "#     dataset_name=\"mnist\",\n",
    "#     t_Epochs=,\n",
    "#     t_Batch_Size=,\n",
    "#     t_Learning_Rate=,\n",
    "#     t_N2R_Ratio=,\n",
    "#     t_Regularization_term=,\n",
    "#     t_Layers=,\n",
    "#     t_Noise_Dim=,\n",
    "#     t_Impair_LR=,\n",
    "#     t_Repair_LR=,\n",
    "#     logs=True,\n",
    "#     model_eval_logs=False,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bach.conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
