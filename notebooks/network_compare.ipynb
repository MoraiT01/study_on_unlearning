{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Networks\n",
    "\n",
    "In this notebook, we want to compare different network to each other.\n",
    "We will start of with...\n",
    "\n",
    "- Untrained Model\n",
    "- Trained Model\n",
    "- Trained Model (Exact Unlearning)*$^1$\n",
    "\n",
    "All these model should serve as comparison point for approximate MU Algorithms, which we want to try out\n",
    "\n",
    "*$^1$ Exact Unlearning: refers to an complete retraining of the Model, without the samples which we want to unlearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data already saved to:  ..\\data\\mnist\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Add the parent directory to sys.path\n",
    "sys.path.append(os.path.join('..', 'src'))\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import copy\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "# from helper import get_model TODO Muss überarbeitet werden\n",
    "from mlp_dataclass import MNIST_CostumDataset, TwoLayerPerceptron\n",
    "from training import main, train_n_models\n",
    "\n",
    "EXAMPLE = os.path.join(\"..\", \"data\", \"mnist_backup\", \"7e\", \"test_36.png\")\n",
    "\n",
    "USED_DATASET = \"mnist\"\n",
    "# OUT OF: [\"mnist\", \"cmnist\", \"fashion_mnist\"]\n",
    "data = MNIST_CostumDataset(\n",
    "    test = True,\n",
    "    dataset_name = USED_DATASET,\n",
    "    download=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sample(path:str):\n",
    "    \"\"\"preps sample to pass to model, transforms to tensor, reshape to 784, shaping it as batch\"\"\"\n",
    "    sample = Image.open(path).convert(\"L\")\n",
    "    sample = torch.Tensor(np.array(sample)).reshape(784)\n",
    "    \n",
    "    return sample.unsqueeze(0)\n",
    "\n",
    "def show_sample(sample:torch.Tensor):\n",
    "    \"\"\"Converts the sample to a numpy array and reshapes it to a 28x28 image\"\"\"\n",
    "    sample = sample.reshape(28, 28)\n",
    "    sample = sample.numpy()\n",
    "    sample = Image.fromarray(sample)\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_sample(prepare_sample(EXAMPLE)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Untrained Model\n",
    "\n",
    "We want to include an untrained model as a pour reference point.\n",
    "\n",
    "Later, we will compare the differently trained (and untrained) models on different metrics. We are thinking, that the **untrained model should always be the furthest neighbor** and see if thats true. Also out of pur curiosity, how do the models compare according to the found metrics to the untrained model.\n",
    "\n",
    "It could be interesting to see, if the unlearned models tend to be more similar to the \"Exact MU Model\" or to the \"Untrained Model\"\n",
    "\n",
    "The \"Trained Model\"´s and \"Exact Model\"´s starting point is the \"Untrained Model\". That means all models can be traced back to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# untrained_model = TwoLayerPerceptron(input_dim=784, output_dim=10)\n",
    "\n",
    "# torch.save(untrained_model.state_dict(), f\"..{os.sep}data{os.sep}models{os.sep}untrained_model\")\n",
    "\n",
    "# print(\"Shape of Tenosr: \", prepare_sample(EXAMPLE).shape)\n",
    "# untrained_model(prepare_sample(EXAMPLE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why not intialize n models\n",
    "# TODO There are no samples in the folder yet\n",
    "# untrained_ms = {f\"untrained_{USED_DATASET}_{v}\": TwoLayerPerceptron(input_dim=data.__getitem__(0)[0].shape[1], output_dim=data.__getitem__(0)[0].shape[1]) for v in range(30)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TwoLayerPerceptron(\n",
       "  (fc1): Linear(in_features=784, out_features=800, bias=True)\n",
       "  (fc3): Linear(in_features=800, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Muss von einem auf Mehrere geändert werden\n",
    "\n",
    "# untrained_model = TwoLayerPerceptron(input_dim=784, output_dim=10)\n",
    "# untrained_model.load_state_dict(torch.load(f\"..{os.sep}data{os.sep}models{os.sep}untrained_model\", weights_only=True))\n",
    "# untrained_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Tenosr:  torch.Size([1, 784])\n",
      "tensor([[-13.3623, -15.0116, -33.1536,  -0.3706, -18.2124, -13.9797, -20.4261,\n",
      "          -1.6220,  -2.1874, -26.4755]], grad_fn=<LogSoftmaxBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([-0.3706], grad_fn=<MaxBackward0>),\n",
       "indices=tensor([3]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Muss von einem auf Mehrere geändert werden\n",
    "\n",
    "# print(\"Shape of Tenosr: \", prepare_sample(EXAMPLE).shape)\n",
    "# print(untrained_model(prepare_sample(EXAMPLE)))\n",
    "# torch.max(untrained_model(prepare_sample(EXAMPLE)), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trained Model\n",
    "\n",
    "This model will be the starting point for all MU algorithms. The more it is important that it does not change throughout the experiments, otherwise we need to run everything again.\n",
    "\n",
    "Notes Regarding the Dataloader: We included Up-/Downsampling. There is a counter in the Dataset class, which controls, which class is next to provide a sample. From the available pool a sample will be chosen at random.\n",
    "\n",
    "Reason: The amount of available samples should not influence the performance of the model on certain classes.\n",
    "\n",
    "Regarding the samples for the class \"7\": There are more samples of sevens without a middle line than ones with.\n",
    "\n",
    "Ratio: \n",
    "- 6268 (train: 5385, test: 883)\n",
    "- 1025 (train: 880,  test: 145)\n",
    "\n",
    "Should there be concerns regarding that the bigger part of the sevens (without middle line) will be unlearning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a single model first\n",
    "main(\n",
    "    new_name=\"trained_showcase\",\n",
    "    model=None,\n",
    "    sampling_mode=\"all\",\n",
    "    balanced_sampling=True,\n",
    "    dataset_name=USED_DATASET,\n",
    "    include_val=True,\n",
    "    logs=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_ms = train_n_models(\n",
    "    sampling_mode=\"except_erased\",\n",
    "    dataset_name=USED_DATASET,\n",
    "    logs=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epoch [1/10] - Train Loss: 10.2791 - Val Loss: 3.7059 - Train Accuracy: 0.7190 - Val Accuracy: 0.8291\n",
    "                                                                   \n",
    "Epoch [2/10] - Train Loss: 3.1310 - Val Loss: 1.9731 - Train Accuracy: 0.8250 - Val Accuracy: 0.8330\n",
    "                                                                   \n",
    "Epoch [3/10] - Train Loss: 1.9117 - Val Loss: 1.6126 - Train Accuracy: 0.8360 - Val Accuracy: 0.8395\n",
    "                                                                   \n",
    "Epoch [4/10] - Train Loss: 1.2980 - Val Loss: 1.4563 - Train Accuracy: 0.8530 - Val Accuracy: 0.8354\n",
    "                                                                   \n",
    "Epoch [5/10] - Train Loss: 0.9279 - Val Loss: 0.9853 - Train Accuracy: 0.8780 - Val Accuracy: 0.8581\n",
    "                                                                   \n",
    "Epoch [6/10] - Train Loss: 0.8566 - Val Loss: 0.5171 - Train Accuracy: 0.8880 - Val Accuracy: 0.9131\n",
    "                                                                   \n",
    "Epoch [7/10] - Train Loss: 0.6793 - Val Loss: 0.4452 - Train Accuracy: 0.9000 - Val Accuracy: 0.9159\n",
    "                                                                   \n",
    "Epoch [8/10] - Train Loss: 0.3773 - Val Loss: 0.3985 - Train Accuracy: 0.9260 - Val Accuracy: 0.9213\n",
    "                                                                   \n",
    "Epoch [9/10] - Train Loss: 0.2953 - Val Loss: 0.3818 - Train Accuracy: 0.9430 - Val Accuracy: 0.9239\n",
    "                                                                   \n",
    "Epoch [10/10] - Train Loss: 0.3581 - Val Loss: 0.3543 - Train Accuracy: 0.9140 - Val Accuracy: 0.9273\n",
    "\n",
    "Model saved to:  ..\\data\\models\\all\\TwoLayerPerceptron_b_trained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Muss von einem auf Mehrere geändert werden\n",
    "\n",
    "# trained_model = get_model(\"trained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exact Unlearned Model\n",
    "\n",
    "This model has the same training conditions as the \"Trained Model\", with the only difference being, that it had no sevens __with no middle line__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a single model first\n",
    "main(\n",
    "    new_name=\"trained_showcase\",\n",
    "    model=None,\n",
    "    sampling_mode=\"except_erased\",\n",
    "    balanced_sampling=True,\n",
    "    dataset_name=USED_DATASET,\n",
    "    include_val=True,\n",
    "    logs=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exact_ms = train_n_models(\n",
    "    sampling_mode=\"except_erased\",\n",
    "    dataset_name=USED_DATASET,\n",
    "    logs=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epoch [1/10] - Train Loss: 11.0494 - Val Loss: 3.9705 - Train Accuracy: 0.6790 - Val Accuracy: 0.8060\n",
    "                                                                   \n",
    "Epoch [2/10] - Train Loss: 2.8843 - Val Loss: 2.0364 - Train Accuracy: 0.8210 - Val Accuracy: 0.8452\n",
    "                                                                   \n",
    "Epoch [3/10] - Train Loss: 1.9582 - Val Loss: 1.9326 - Train Accuracy: 0.8260 - Val Accuracy: 0.8066\n",
    "                                                                   \n",
    "Epoch [4/10] - Train Loss: 1.0714 - Val Loss: 0.6197 - Train Accuracy: 0.8630 - Val Accuracy: 0.9021\n",
    "                                                                   \n",
    "Epoch [5/10] - Train Loss: 0.9799 - Val Loss: 0.7909 - Train Accuracy: 0.8540 - Val Accuracy: 0.8872\n",
    "                                                                   \n",
    "Epoch [6/10] - Train Loss: 0.5125 - Val Loss: 0.5123 - Train Accuracy: 0.9140 - Val Accuracy: 0.9135\n",
    "                                                                   \n",
    "Epoch [7/10] - Train Loss: 0.3382 - Val Loss: 0.4401 - Train Accuracy: 0.9360 - Val Accuracy: 0.9180\n",
    "                                                                   \n",
    "Epoch [8/10] - Train Loss: 0.6186 - Val Loss: 0.3889 - Train Accuracy: 0.8990 - Val Accuracy: 0.9229\n",
    "                                                                   \n",
    "Epoch [9/10] - Train Loss: 0.4423 - Val Loss: 0.3394 - Train Accuracy: 0.9170 - Val Accuracy: 0.9277\n",
    "                                                                   \n",
    "Epoch [10/10] - Train Loss: 0.4092 - Val Loss: 0.3211 - Train Accuracy: 0.9370 - Val Accuracy: 0.9291\n",
    "\n",
    "Model saved to:  ..\\data\\models\\except_erased\\TwoLayerPerceptron_b_exact_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Muss von einem auf Mehrere geändert werden\n",
    "\n",
    "# exact_model = get_model(\"exact\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics import calc_accuracy\n",
    "from torch.utils.data import DataLoader\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2 Norm\n",
    "\n",
    "Compare how the different models are from each other using the L2 Norm, comparing every parameter with each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics import model_l2_norm_difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unlearn to ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...trained model\n",
    "a11 = model_l2_norm_difference(untrained_model, trained_model)\n",
    "sum(a11.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...exact model\n",
    "a12 = model_l2_norm_difference(untrained_model, exact_model)\n",
    "sum(a12.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trained to ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...untrained model\n",
    "a21 = model_l2_norm_difference(trained_model, untrained_model)\n",
    "sum(a21.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...exact model\n",
    "a22 = model_l2_norm_difference(trained_model, exact_model)\n",
    "sum(a22.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exact to ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...untrained model\n",
    "model_l2_norm_difference(exact_model, untrained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...trained model\n",
    "model_l2_norm_difference(exact_model, trained_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KL-Divergence\n",
    "\n",
    "Used to compare the difference between distribution, but may also be used to compare models by their predicted target distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics import kl_divergence_between_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unlearned to..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trained to..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exact to..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy\n",
    "\n",
    "- $D_{Gesamt}$\n",
    "- $D_{Erased}$\n",
    "- $D_{Remain}$\n",
    "- $D_{Accuracy Per Class}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop Criterium von 10 nicht fair, da es sich implizit \n",
    "\n",
    "- Wie verhält sich das Unlearning über die Anzahl der Epochen?\n",
    "\n",
    "- Man sollte sich den Loss als Referenz anschauen\n",
    "\n",
    "- Festes Downsampling, wenn die 7 reduzierte Samples haben, müssten auch die anderen Klassen weniger Samples in der Grundmenge haben.\n",
    "\n",
    "- auf einen weiteren Datensatz soll es auch getestet werden. (C_MNIST wie robust ist der Algorithmus gegenüber Rausche)\n",
    "    - ggf. auf weiterem Datensatz ein Merkmal versuchen zu Unlearnen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualize import boxplotting_multimodel_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us see what metrics each model has"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Macht vllt ned so viel Sinn, fand i nur witzig\n",
    "# boxplotting_multimodel_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplotting_multimodel_eval(trained_ms, dataset_name=USED_DATASET, evaluation=\"accuracy\", logs=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplotting_multimodel_eval(exact_ms, dataset_name=USED_DATASET, evaluation=\"accuracy\", logs=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bach.conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
