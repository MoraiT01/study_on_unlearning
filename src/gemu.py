"""
    contains all the code necessary for running the MU algorithm inspired by https://github.com/vikram2000b/Fast-Machine-Unlearning
    Instead of unlearning one entire class, we focus on unlearning a subset of on class, grouped together by one shared feature
        -> hence: Feature Unlearning
"""

# import required libraries
import numpy as np

from typing import Literal, Tuple, Dict
import torch
from torch import nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, Dataset

# my
from training import model_params
from mlp_dataclass import MNIST_CostumDataset
from my_random import shared_random_state
import math
import datetime
from tqdm import tqdm

torch.manual_seed(100)

DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
NOISE_BATCH_SIZE = 256

def accuracy(outputs, labels):
    _, preds = torch.max(outputs, dim=1)
    _, l = torch.max(labels, dim=1)
    correct = torch.sum(preds == l).item()
    return torch.tensor(correct / len(preds))

def validation_step(model, batch):
    images, labels = batch
    images, labels = images.to(DEVICE), labels.to(DEVICE)
    out = model(images)                    
    loss = F.cross_entropy(out, labels)   
    acc = accuracy(out, labels)
    return {'Loss': loss.detach(), 'Acc': acc}

def validation_epoch_end(model, outputs):
    batch_losses = [x['Loss'] for x in outputs]
    epoch_loss = torch.stack(batch_losses).mean()   
    batch_accs = [x['Acc'] for x in outputs]
    epoch_acc = torch.stack(batch_accs).mean()      
    return {'Loss': epoch_loss.item(), 'Acc': epoch_acc.item()}

@torch.no_grad()
def evaluate(model, val_loader):
    model.eval()
    outputs = [validation_step(model, batch) for batch in val_loader]
    return validation_epoch_end(model, outputs)

# defining the noise structure
class Noise(nn.Module):
    def __init__(self, *dim):
        super().__init__()
        self.noise = torch.nn.Parameter(torch.randn(*dim), requires_grad = True)
        
    def forward(self):
        return self.noise

def prep_noise_generator(forget_data: Dataset, model: torch.nn.Module) -> Tuple[Noise, Dict, Dict]:
    """

    """
    
    noises = {}
    og_labels = {}
    created_labels =  {}
    model.eval()
    for index, data in enumerate(DataLoader(forget_data, batch_size=1, shuffle=False)): # iterate over forget_data):
        s, l = data

        new_l = F.softmax(model(s.to(DEVICE)).detach(), dim=1)
        
        created_labels[index] = new_l[0].to(DEVICE)
        og_labels[index] = l[0].to(DEVICE)
        # UrsprÃ¼nglich waren hier die Labels der der Klassen gemeint
        # Jedoch entschied ich mich dagegen
        # Der prognostizierte Wahrkeitsvektor ist eine andere Darstellung des Samples,
        # Wir wollen nicht die Klasse unlearnen, sonder das Sample/das Feature

    batch_size = [NOISE_BATCH_SIZE]
    batch_size.extend(s[0].shape)
    noises = Noise(batch_size).to(DEVICE)

    return noises, created_labels, og_labels

def noise_maximization(forget_data: Dataset, model: torch.nn.Module, logs: bool = False) -> Tuple[Noise, Dict[int, torch.Tensor] | torch.Tensor]:
    """

    """
    noise_batch, _, og_labels = prep_noise_generator(forget_data, model)
    # Should be cls label 7 for all of them
    # Unnecessary, I guess but also works
    cls_labels = torch.stack(list(og_labels.values()))
    cls_mean = torch.mean(cls_labels, dim=0)
    cls_index = torch.argmax(cls_mean)
    cls_label = torch.zeros_like(cls_mean)
    cls_label[cls_index] = 1

    label_batch = torch.stack([cls_label for _ in range(NOISE_BATCH_SIZE)]).to(DEVICE)

    opt = torch.optim.Adam(noise_batch.parameters(), lr = 0.1)

    num_epochs = 5
    num_steps = 8

    for epoch in range(num_epochs):
        total_loss = []
        for batch in range(num_steps):
            inputs = noise_batch()
            
            outputs = model(inputs)
            loss = -F.cross_entropy(outputs, label_batch) + 0.1*torch.mean(torch.sum(torch.square(inputs), [1]))
            opt.zero_grad()
            loss.backward()
            opt.step()
            total_loss.append(loss.cpu().detach().numpy())
        if logs:
            print("Loss: {}".format(np.mean(total_loss)))
    
    return noise_batch, cls_label

class FeatureMU_Loader(Dataset):
    """
    This class creates a new dataset which contains the given retain_data and some noise generated by the noise_generator.
    The noise is generated with the label4noise and added to the dataset as many times as specified in number_of_noise.
    """

    def __init__(self, noise_generator: Noise, label4noise: Dict[int, torch.Tensor] | torch.Tensor, number_of_noise: int, retain_data: Dataset):
        """

        """
        self.noise_gen = noise_generator
        self.retain_data = retain_data
        self.number_of_noise = number_of_noise
        self.label4noise = label4noise

        self.noise_gen.eval()

    def __len__(self) -> int:
        """

        """
        return len(self.retain_data) + self.number_of_noise

    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:
        """

        """
        if idx < self.number_of_noise:
            label = self.label4noise if isinstance(self.label4noise, torch.Tensor) else self.label4noise[idx]
            return self.noise_gen()[idx], label
        else:
            return self.retain_data.__getitem__(idx - self.number_of_noise)

def impairing_phase(noise_batch: Noise, number_of_noise: int, label4noise: torch.Tensor, retain_data: Dataset, model: torch.nn.Module, logs: bool = False) -> torch.nn.Module:
    """

    """
    noisy_loader = DataLoader(
        dataset=FeatureMU_Loader(noise_batch, label4noise, number_of_noise, retain_data),
        batch_size=8, 
        shuffle=True
    )
    optimizer = torch.optim.Adam(model.parameters(), lr = 0.02) # Hyperparameter

    for epoch in range(1): # Hyperparameter  
        model.train(True)
        running_loss = 0.0
        
        for i, data in enumerate(noisy_loader):
            inputs, labels = data
            inputs, labels = inputs.to(DEVICE), labels.clone().detach().to(DEVICE)

            optimizer.zero_grad()
            outputs = model(inputs)
            loss = F.cross_entropy(outputs, labels)
            loss.backward()
            optimizer.step()

            # Append the loss to the list of losses
            running_loss += loss.item() * inputs.size(0)

        if logs:
            print(f"Train loss {epoch+1}: {running_loss/len(noisy_loader)}")

    return model

def repairing_phase(retain_data: Dataset, model: torch.nn.Module, logs: bool = False) -> torch.nn.Module:
    """
    Perform the repairing phase of the Fast Machine Unlearning algorithm.

    Parameters:
        retain_data (Dataset): The retained data.
        model (torch.nn.Module): The model to be repaired.
        logs (bool, optional): Whether to print logs. Defaults to False.

    Returns:
        torch.nn.Module: The model after the repairing phase.
    """
    heal_loader = torch.utils.data.DataLoader(retain_data, batch_size=8, shuffle = True)

    optimizer = torch.optim.Adam(model.parameters(), lr = 0.01) # Hyperparameter

    for epoch in range(1): # Hyperparameter
        model.train(True)
        running_loss = 0.0
        
        for i, data in enumerate(heal_loader):
            inputs, labels = data
            inputs, labels = inputs.to(DEVICE), labels.clone().detach().to(DEVICE)

            optimizer.zero_grad()
            outputs = model(inputs)
            loss = F.cross_entropy(outputs, labels)
            loss.backward()
            optimizer.step()
            
            # Append the loss to the list of losses
            running_loss += loss.item() * inputs.size(0)

        if logs:
            print(f"Train loss {epoch+1}: {running_loss/len(heal_loader)}")

    return model

def _main(
        model: torch.nn.Module,
        dataset_name: Literal["mnist", "cmnist", "fashion_mnist"],
        logs: bool = False,
    ) -> torch.nn.Module:
    """
    Main function for the Fast Machine Unlearning inspired algorithm.

    Parameters:
        model (torch.nn.Module): The model to be unlearned.
        dataset_name (Literal["mnist", "cmnist", "fashion_mnist"]): The name of the dataset to use.
        logs (bool, optional): Whether to print logs. Defaults to False.

    Returns:
        torch.nn.Module: The model after the unlearning process.
    """
    model.to(DEVICE)
    start_time = datetime.datetime.now().timestamp()

    # Validation Dataloaders for the forget data
    forget_valid_dl = DataLoader(
        dataset = MNIST_CostumDataset(
            sample_mode="only_erased",
            train=False,
            test=True,
            dataset_name=dataset_name,
        ),
        batch_size=8,
        shuffle=False,
    )
    # Validation Dataloader for the retained data
    retain_valid_dl = DataLoader(
        dataset = MNIST_CostumDataset(
            sample_mode="except_erased",
            train=False,
            test=True,
            dataset_name=dataset_name,
        ),
        batch_size=8,
        shuffle=False,
    )
    # the forget train data
    data_forget = MNIST_CostumDataset(
        sample_mode="only_erased",
        train=True,
        test=False,
        dataset_name=dataset_name,
    )
    
    if logs:
        print("Baseline Performance")
        print("Time: {}".format(datetime.datetime.now().timestamp() - start_time))

        print("______")
        print("Performance of Baseline on Forget Class")
        history = [evaluate(model, forget_valid_dl)]
        print("Accuracy: {}".format(history[0]["Acc"]*100))
        print("Loss: {}".format(history[0]["Loss"]))

        print("Performance of Baseline Model on Retain Class")
        history = [evaluate(model, retain_valid_dl)]
        print("Accuracy: {}".format(history[0]["Acc"]*100))
        print("Loss: {}".format(history[0]["Loss"]))

        print("______")
        print("Starting Noise Maximazation Phase")
        print("Time: {}".format(datetime.datetime.now().timestamp() - start_time))
    
    # (Re)Training Dataloaders
    noise_gen, noise_labels = noise_maximization(
        forget_data=data_forget,
        model=model,
        logs=logs,
    )

    if logs:
        print("Ending Noise Maximazation Phase")
        print("Time: {}".format(datetime.datetime.now().timestamp() - start_time))

    # the retain train data
    retain_data = MNIST_CostumDataset(
            sample_mode="except_erased",
            train=True,
            test=False,
            balanced=True,
            dataset_name=dataset_name,
        )
    
    # We need to make sure that the cls are balanced
    # take the same amout like in the paper of femu
    # 1000 samples per other class
    retain_data.length = 1000 * 9 # every except the one we want to forget from
    if logs:
        print("______")
        print("Starting Impairing Phase")
        print("Time: {}".format(datetime.datetime.now().timestamp() - start_time))
    
    impaired_model = impairing_phase(
        noise_generator=noise_gen,
        number_of_noise=NOISE_BATCH_SIZE,
        label4noise=noise_labels,
        retain_data=retain_data,
        model=model,
        logs=logs,
    )

    if logs:
        print("Ending Impairing Phase")
        print("Time: {}".format(datetime.datetime.now().timestamp() - start_time))

        print("______")
        print("Performance of Impaired Model on Forget Class")
        history = [evaluate(impaired_model, forget_valid_dl)]
        print("Accuracy: {}".format(history[0]["Acc"]*100))
        print("Loss: {}".format(history[0]["Loss"]))

        print("Performance of Impaired Model on Retain Class")
        history = [evaluate(impaired_model, retain_valid_dl)]
        print("Accuracy: {}".format(history[0]["Acc"]*100))
        print("Loss: {}".format(history[0]["Loss"]))

        print("______")
        print("Starting Repairing Phase")
        print("Time: {}".format(datetime.datetime.now().timestamp() - start_time))
    
    repaired_model = repairing_phase(
        retain_data=retain_data,
        model=impaired_model,
        logs=logs,
    )

    if logs:
        print("Ending Repairing Phase")
        print("Time: {}".format(datetime.datetime.now().timestamp() - start_time))
        print("______")

        print("Performance of repaired Model on Forget Class")
        history = [evaluate(repaired_model, forget_valid_dl)]
        print("Accuracy: {}".format(history[0]["Acc"]*100))
        print("Loss: {}".format(history[0]["Loss"]))

        print("Performance of Repaired Model on Retain Class")
        history = [evaluate(repaired_model, retain_valid_dl)]
        print("Accuracy: {}".format(history[0]["Acc"]*100))
        print("Loss: {}".format(history[0]["Loss"]))

        print("Time: {}".format(datetime.datetime.now().timestamp() - start_time))

    return repaired_model

